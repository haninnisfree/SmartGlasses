"""
ÌååÏùº ÏóÖÎ°úÎìú API ÎùºÏö∞ÌÑ∞
MODIFIED 2024-01-20: ÌååÏùº raw text Ï°∞Ìöå Î∞è Ìé∏Ïßë Í∏∞Îä• Ï∂îÍ∞Ä
ENHANCED 2024-01-21: ÌååÏùº ÎØ∏Î¶¨Î≥¥Í∏∞ Í∏∞Îä• Ï∂îÍ∞Ä
REFACTORED 2024-01-21: Ï§ëÎ≥µ Í≤ÄÏÉâ API Ï†úÍ±∞ Î∞è ÏΩîÎìú Ï†ïÎ¶¨
FIXED 2024-01-21: import Í≤ΩÎ°ú ÏàòÏ†ï (file_processing -> data_processing)
FIXED 2025-06-03: DocumentProcessor Ï¥àÍ∏∞Ìôî Î∞è Î©îÏÑúÎìú Ìò∏Ï∂ú Ïò§Î•ò ÏàòÏ†ï
"""
import os
import time
import uuid
from datetime import datetime
from pathlib import Path
from typing import List, Optional

from fastapi import APIRouter, File, Form, HTTPException, UploadFile
from fastapi.responses import JSONResponse
from pydantic import BaseModel

from database.connection import get_database
from data_processing.document_processor import DocumentProcessor
from retrieval.vector_search import VectorSearch
from utils.logger import get_logger

logger = get_logger(__name__)
router = APIRouter()

class UploadResponse(BaseModel):
    """ÏóÖÎ°úÎìú ÏùëÎãµ Î™®Îç∏"""
    success: bool
    message: str
    file_id: str
    original_filename: str
    processed_chunks: int
    storage_path: Optional[str] = None

class FileStatus(BaseModel):
    """ÌååÏùº ÏÉÅÌÉú Î™®Îç∏"""
    file_id: str
    original_filename: str
    file_type: str
    file_size: int
    status: str  # 'uploading', 'processing', 'completed', 'failed'
    processed_chunks: int
    upload_time: datetime
    folder_id: Optional[str] = None

class FileSearchRequest(BaseModel):
    """ÌååÏùº Í≤ÄÏÉâ ÏöîÏ≤≠ Î™®Îç∏"""
    query: str
    search_type: str = "both"  # filename, content, both
    folder_id: Optional[str] = None
    limit: int = 20
    skip: int = 0

class FileSearchResult(BaseModel):
    """ÌååÏùº Í≤ÄÏÉâ Í≤∞Í≥º Î™®Îç∏"""
    file_id: str
    original_filename: str
    file_type: str
    file_size: int
    processed_chunks: int
    upload_time: datetime
    folder_id: Optional[str] = None
    description: Optional[str] = None
    match_type: str  # filename, content, both
    relevance_score: float
    matched_content: Optional[str] = None  # Í≤ÄÏÉâÏñ¥ÏôÄ Îß§Ïπ≠Îêú ÎÇ¥Ïö© ÎØ∏Î¶¨Î≥¥Í∏∞

class FileSearchResponse(BaseModel):
    """ÌååÏùº Í≤ÄÏÉâ ÏùëÎãµ Î™®Îç∏"""
    files: List[FileSearchResult]
    total_found: int
    query: str
    search_type: str
    execution_time: float

class FileUpdateRequest(BaseModel):
    """ÌååÏùº Ï†ïÎ≥¥ ÏóÖÎç∞Ïù¥Ìä∏ ÏöîÏ≤≠ Î™®Îç∏"""
    filename: Optional[str] = None
    description: Optional[str] = None
    folder_id: Optional[str] = None
    folder_title: Optional[str] = None

class FilePreviewResponse(BaseModel):
    """ÌååÏùº ÎØ∏Î¶¨Î≥¥Í∏∞ ÏùëÎãµ Î™®Îç∏"""
    file_id: str
    original_filename: str
    file_type: str
    preview_text: str
    preview_length: int
    total_length: int
    has_more: bool
    preview_type: str  # "text", "pdf_extract", "document_extract"

@router.post("/", response_model=UploadResponse)
async def upload_file(
    file: UploadFile = File(...),
    folder_id: Optional[str] = Form(None),
    folder_title: Optional[str] = Form(None),
    description: Optional[str] = Form(None)
):
    """ÌååÏùº ÏóÖÎ°úÎìú Î∞è Ï≤òÎ¶¨"""
    try:
        # ÎîîÎ≤ÑÍπÖ: Î∞õÏùÄ Ìèº Îç∞Ïù¥ÌÑ∞ Î°úÍ∑∏ Ï∂úÎ†•
        logger.info(f"ÏóÖÎ°úÎìú Ìèº Îç∞Ïù¥ÌÑ∞ - ÌååÏùºÎ™Ö: {file.filename}, folder_id: '{folder_id}', folder_title: '{folder_title}', description: '{description}'")
        
        # folder_idÏôÄ folder_title ÎèôÏãú ÏûÖÎ†• Î∞©ÏßÄ
        if (folder_id and folder_id.strip() and folder_id not in ["string", "null"]) and \
           (folder_title and folder_title.strip() and folder_title not in ["string", "null"]):
            raise HTTPException(
                status_code=400, 
                detail="folder_idÏôÄ folder_titleÏùÄ ÎèôÏãúÏóê ÏûÖÎ†•Ìï† Ïàò ÏóÜÏäµÎãàÎã§. Îëò Ï§ë ÌïòÎÇòÎßå ÏÑ†ÌÉùÌï¥Ï£ºÏÑ∏Ïöî."
            )
        
        # ÌååÏùº Ï†ïÎ≥¥ Í≤ÄÏ¶ù
        if not file.filename:
            raise HTTPException(status_code=400, detail="ÌååÏùºÎ™ÖÏù¥ ÏóÜÏäµÎãàÎã§.")
        
        # ÌååÏùº ÌÉÄÏûÖ ÌôïÏù∏
        allowed_types = ['.txt', '.pdf', '.docx', '.doc', '.md']
        file_ext = os.path.splitext(file.filename)[1].lower()
        if file_ext not in allowed_types:
            raise HTTPException(
                status_code=400, 
                detail=f"ÏßÄÏõêÌïòÏßÄ ÏïäÎäî ÌååÏùº ÌòïÏãùÏûÖÎãàÎã§. ÏßÄÏõê ÌòïÏãù: {', '.join(allowed_types)}"
            )
        
        # ÌååÏùº ÌÅ¨Í∏∞ ÌôïÏù∏ (10MB Ï†úÌïú)
        file_content = await file.read()
        if len(file_content) > 10 * 1024 * 1024:  # 10MB
            raise HTTPException(status_code=400, detail="ÌååÏùº ÌÅ¨Í∏∞Îäî 10MBÎ•º Ï¥àÍ≥ºÌï† Ïàò ÏóÜÏäµÎãàÎã§.")
        
        # ÌååÏùºÏùÑ Îã§Ïãú Ï≤òÏùåÏúºÎ°ú ÎêòÎèåÎ¶¨Í∏∞
        await file.seek(0)
        
        # Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïó∞Í≤∞
        db = await get_database()
        
        # ÌååÏùº ID ÏÉùÏÑ±
        file_id = str(uuid.uuid4())
        
        # ÏûÑÏãú ÌååÏùºÎ°ú Ï†ÄÏû•
        upload_dir = Path("uploads")
        upload_dir.mkdir(exist_ok=True)
        
        temp_filename = f"{file_id}_{file.filename}"
        temp_file_path = upload_dir / temp_filename
        
        with open(temp_file_path, "wb") as temp_file:
            temp_file.write(file_content)
        
        # Form Îç∞Ïù¥ÌÑ∞ Ï†ïÎ¶¨ Î∞è Ìè¥Îçî ID Í≤∞Ï†ï
        clean_folder_id = None
        clean_description = None
        
        # 1. folder_id ÏßÅÏ†ë ÏûÖÎ†• Ï≤òÎ¶¨
        if folder_id and folder_id.strip() and folder_id not in ["string", "null"]:
            clean_folder_id = folder_id.strip()
            logger.info(f"folder_idÎ°ú Ìè¥Îçî ÏßÄÏ†ï: {clean_folder_id}")
            
            # folder_id Ïú†Ìö®ÏÑ± Í≤ÄÏ¶ù
            from bson import ObjectId
            try:
                if not ObjectId.is_valid(clean_folder_id):
                    raise HTTPException(status_code=400, detail="Ïú†Ìö®ÌïòÏßÄ ÏïäÏùÄ folder_id ÌòïÏãùÏûÖÎãàÎã§.")
                
                # Ìè¥Îçî Ï°¥Ïû¨ ÌôïÏù∏
                folder_exists = await db.folders.find_one({"_id": ObjectId(clean_folder_id)})
                if not folder_exists:
                    raise HTTPException(status_code=404, detail=f"folder_id '{clean_folder_id}'Ïóê Ìï¥ÎãπÌïòÎäî Ìè¥ÎçîÎ•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.")
                    
            except Exception as e:
                if isinstance(e, HTTPException):
                    raise
                raise HTTPException(status_code=400, detail=f"folder_id Í≤ÄÏ¶ù Ïã§Ìå®: {str(e)}")
        
        # 2. folder_titleÎ°ú Ìè¥Îçî Í≤ÄÏÉâ Ï≤òÎ¶¨
        elif folder_title and folder_title.strip() and folder_title not in ["string", "null"]:
            clean_folder_title = folder_title.strip()
            logger.info(f"folder_titleÎ°ú Ìè¥Îçî Í≤ÄÏÉâ: {clean_folder_title}")
            
            # Ìè¥Îçî titleÎ°ú Í≤ÄÏÉâ
            folder_by_title = await db.folders.find_one({"title": clean_folder_title})
            if not folder_by_title:
                raise HTTPException(
                    status_code=404, 
                    detail=f"'{clean_folder_title}' Ï†úÎ™©Ïùò Ìè¥ÎçîÎ•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§. Î®ºÏ†Ä Ìè¥ÎçîÎ•º ÏÉùÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî."
                )
            
            clean_folder_id = str(folder_by_title["_id"])
            logger.info(f"Ìè¥Îçî title '{clean_folder_title}' -> folder_id: {clean_folder_id}")
        
        # 3. description Ï≤òÎ¶¨
        if description and description.strip() and description not in ["string", "null"]:
            clean_description = description.strip()
        
        logger.info(f"ÏµúÏ¢Ö Ï†ïÎ¶¨Îêú Îç∞Ïù¥ÌÑ∞ - clean_folder_id: '{clean_folder_id}', clean_description: '{clean_description}'")
        
        # ÌååÏùº Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ
        file_metadata = {
            "file_id": file_id,
            "original_filename": file.filename,
            "file_type": file_ext[1:],  # Ï†ê Ï†úÍ±∞
            "file_size": len(file_content),
            "upload_time": datetime.utcnow(),
            "folder_id": clean_folder_id,
            "description": clean_description
        }
        
        # Î¨∏ÏÑú Ï≤òÎ¶¨Í∏∞Î°ú ÌååÏùº Ï≤òÎ¶¨
        processor = DocumentProcessor(db)
        result = await processor.process_and_store(
            file_path=temp_file_path,
            file_metadata=file_metadata
        )
        
        # ÏûÑÏãú ÌååÏùº ÏÇ≠Ï†ú
        try:
            temp_file_path.unlink()
        except Exception as e:
            logger.warning(f"ÏûÑÏãú ÌååÏùº ÏÇ≠Ï†ú Ïã§Ìå®: {e}")
        
        # ÏÑ±Í≥µ Î©îÏãúÏßÄ ÏÉùÏÑ±
        success_message = "ÌååÏùº ÏóÖÎ°úÎìúÍ∞Ä ÏôÑÎ£åÎêòÏóàÏäµÎãàÎã§."
        if clean_folder_id:
            # Ìè¥Îçî Ï†ïÎ≥¥ Ï°∞ÌöåÌï¥ÏÑú Î©îÏãúÏßÄÏóê Ìè¨Ìï®
            try:
                folder_info = await db.folders.find_one({"_id": ObjectId(clean_folder_id)})
                if folder_info:
                    success_message += f" (Ìè¥Îçî: {folder_info['title']})"
            except:
                pass
        
        logger.info(f"ÌååÏùº ÏóÖÎ°úÎìú ÏôÑÎ£å: {file.filename} -> {file_id} (Ìè¥Îçî: {clean_folder_id})")
        
        return UploadResponse(
            success=True,
            message=success_message,
            file_id=file_id,
            original_filename=file.filename,
            processed_chunks=result["chunks_count"],
            storage_path=str(temp_file_path)
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ÌååÏùº ÏóÖÎ°úÎìú Ïã§Ìå®: {e}")
        raise HTTPException(status_code=500, detail=f"ÌååÏùº ÏóÖÎ°úÎìú Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {str(e)}")

@router.get("/status/{file_id}", response_model=FileStatus)
async def get_file_status(file_id: str):
    """ÌååÏùº Ï≤òÎ¶¨ ÏÉÅÌÉú Ï°∞Ìöå"""
    try:
        db = await get_database()
        
        # 1. file_info Ïª¨Î†âÏÖòÏóêÏÑú ÌååÏùº Ï†ïÎ≥¥ Ï°∞Ìöå (Ï≤òÎ¶¨ ÏÉÅÌÉú Ìè¨Ìï®)
        file_info = await db.file_info.find_one({"file_id": file_id})
        
        if file_info:
            # file_infoÏóê Í∏∞Î°ùÏù¥ ÏûàÏúºÎ©¥ Ìï¥Îãπ ÏÉÅÌÉú Î∞òÌôò
            chunks_count = await db.chunks.count_documents({"file_id": file_id})
            
            return FileStatus(
                file_id=file_id,
                original_filename=file_info["original_filename"],
                file_type=file_info["file_type"],
                file_size=file_info["file_size"],
                status=file_info["processing_status"],  # "processing", "completed", "failed"
                processed_chunks=chunks_count,
                upload_time=file_info["upload_time"],
                folder_id=file_info.get("folder_id")
            )
        
        # 2. documents Ïª¨Î†âÏÖòÏóêÏÑú ÌååÏùº Ï†ïÎ≥¥ Ï°∞Ìöå (ÏÉàÎ°úÏö¥ Íµ¨Ï°∞)
        document = await db.documents.find_one({"file_metadata.file_id": file_id})
        
        if document:
            # documentsÏóê ÏûàÏúºÎ©¥ ÏÑ±Í≥µÏ†ÅÏúºÎ°ú Ï≤òÎ¶¨Îêú Í≤É
            chunks_count = await db.chunks.count_documents({"file_id": file_id})
            file_metadata = document["file_metadata"]
            
            return FileStatus(
                file_id=file_id,
                original_filename=file_metadata["original_filename"],
                file_type=file_metadata["file_type"],
                file_size=file_metadata["file_size"],
                status="completed",  # documentsÏóê ÏûàÏúºÎ©¥ Ï≤òÎ¶¨ ÏôÑÎ£å
                processed_chunks=chunks_count,
                upload_time=document["created_at"],
                folder_id=document.get("folder_id")
            )
        
        # 3. chunks Ïª¨Î†âÏÖòÏóêÏÑú ÏßÅÏ†ë Ï°∞Ìöå (Î†àÍ±∞Ïãú Ìò∏ÌôòÏÑ±)
        chunk = await db.chunks.find_one({"file_id": file_id})
        
        if chunk:
            # chunksÏóêÎßå ÏûàÎäî Í≤ΩÏö∞ (Î†àÍ±∞Ïãú Îç∞Ïù¥ÌÑ∞)
            chunks_count = await db.chunks.count_documents({"file_id": file_id})
            metadata = chunk.get("metadata", {})
            
            return FileStatus(
                file_id=file_id,
                original_filename=metadata.get("source", "Ïïå Ïàò ÏóÜÎäî ÌååÏùº"),
                file_type=metadata.get("file_type", "unknown"),
                file_size=0,  # chunksÏóêÏÑúÎäî ÌååÏùº ÌÅ¨Í∏∞ Ï†ïÎ≥¥Í∞Ä ÏóÜÏùå
                status="completed",  # chunksÏóê ÏûàÏúºÎ©¥ Ï≤òÎ¶¨ ÏôÑÎ£åÎ°ú Í∞ÑÏ£º
                processed_chunks=chunks_count,
                upload_time=chunk.get("created_at", datetime.utcnow()),
                folder_id=chunk.get("folder_id")
            )
        
        # 4. Ïñ¥ÎîîÏóêÎèÑ ÏóÜÏúºÎ©¥ 404 ÏóêÎü¨
        raise HTTPException(status_code=404, detail="ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.")
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ÌååÏùº ÏÉÅÌÉú Ï°∞Ìöå Ïã§Ìå®: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/search", response_model=FileSearchResponse)
async def search_files(request: FileSearchRequest):
    """üìç ÏûêÏó∞Ïñ¥ ÌååÏùº Í≤ÄÏÉâ - ÌååÏùºÎ™ÖÍ≥º ÎÇ¥Ïö©ÏúºÎ°ú Í≤ÄÏÉâ Í∞ÄÎä•"""
    try:
        start_time = time.time()
        db = await get_database()
        
        # 1. Í∏∞Î≥∏ ÌïÑÌÑ∞ Ï°∞Í±¥ ÏÑ§Ï†ï (folder_idÍ∞Ä Ïã§Ï†ú Í∞íÏùº ÎïåÎßå Ï†ÅÏö©)
        base_filter = {}
        if request.folder_id and request.folder_id.strip() and request.folder_id != "string":
            base_filter["folder_id"] = request.folder_id
        
        found_files = []
        
        # 2. ÌååÏùºÎ™Ö Í≤ÄÏÉâ (filename ÎòêÎäî both)
        if request.search_type in ["filename", "both"]:
            filename_filter = base_filter.copy()
            # ÏÉàÎ°úÏö¥ Íµ¨Ï°∞: file_metadata.original_filenameÏóêÏÑú Í≤ÄÏÉâ
            filename_filter["file_metadata.original_filename"] = {"$regex": request.query, "$options": "i"}
            
            filename_docs = await db.documents.find(filename_filter).to_list(None)
            
            for doc in filename_docs:
                file_metadata = doc.get("file_metadata", {})
                file_id = file_metadata.get("file_id")
                
                if not file_id:
                    continue
                    
                chunks_count = doc.get("chunks_count", 0)  # Ï†ÄÏû•Îêú ÌÜµÍ≥Ñ ÏÇ¨Ïö©
                
                found_files.append({
                    "file_id": file_id,
                    "original_filename": file_metadata.get("original_filename", "Ïïå Ïàò ÏóÜÎäî ÌååÏùº"),
                    "file_type": file_metadata.get("file_type", "unknown"),
                    "file_size": file_metadata.get("file_size", 0),
                    "processed_chunks": chunks_count,
                    "upload_time": doc.get("created_at", datetime.utcnow()),
                    "folder_id": doc.get("folder_id"),
                    "description": file_metadata.get("description"),
                    "match_type": "filename",
                    "relevance_score": 1.0,  # ÌååÏùºÎ™Ö Îß§ÏπòÎäî ÎÜíÏùÄ Ï†êÏàò
                    "matched_content": f"ÌååÏùºÎ™Ö Îß§Ïπò: {file_metadata.get('original_filename')}"
                })
        
        # 3. ÎÇ¥Ïö© Í≤ÄÏÉâ (content ÎòêÎäî both) - ÏÉàÎ°úÏö¥ Íµ¨Ï°∞Ïóê ÎßûÍ≤å ÏàòÏ†ï
        if request.search_type in ["content", "both"]:
            content_filter = base_filter.copy()
            content_filter["raw_text"] = {"$regex": request.query, "$options": "i"}
            
            content_docs = await db.documents.find(content_filter).to_list(None)
            
            for doc in content_docs:
                file_metadata = doc.get("file_metadata", {})
                file_id = file_metadata.get("file_id")
                
                if not file_id:
                    continue
                
                # Ïù¥ÎØ∏ ÌååÏùºÎ™ÖÏúºÎ°ú Ï∞æÏùÄ Í≤ΩÏö∞ Ïä§ÌÇµ (Ï§ëÎ≥µ Î∞©ÏßÄ)
                if any(f["file_id"] == file_id for f in found_files):
                    continue
                
                chunks_count = doc.get("chunks_count", 0)
                raw_text = doc.get("raw_text", "")
                
                # Îß§Ïπ≠Îêú ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú (Í∞ÑÎã®Ìïú ÌïòÏù¥ÎùºÏù¥Ìä∏)
                query_lower = request.query.lower()
                text_lower = raw_text.lower()
                
                # Í≤ÄÏÉâÏñ¥ Ï£ºÎ≥Ä ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú
                match_index = text_lower.find(query_lower)
                if match_index != -1:
                    start = max(0, match_index - 50)
                    end = min(len(raw_text), match_index + len(request.query) + 50)
                    matched_content = "..." + raw_text[start:end] + "..."
                else:
                    matched_content = raw_text[:100] + "..."
                
                found_files.append({
                    "file_id": file_id,
                    "original_filename": file_metadata.get("original_filename", "Ïïå Ïàò ÏóÜÎäî ÌååÏùº"),
                    "file_type": file_metadata.get("file_type", "unknown"),
                    "file_size": file_metadata.get("file_size", 0),
                    "processed_chunks": chunks_count,
                    "upload_time": doc.get("created_at", datetime.utcnow()),
                    "folder_id": doc.get("folder_id"),
                    "description": file_metadata.get("description"),
                    "match_type": "content",
                    "relevance_score": 0.8,  # ÎÇ¥Ïö© Îß§ÏπòÎäî Ï§ëÍ∞Ñ Ï†êÏàò
                    "matched_content": matched_content
                })
        
        # 4. Í¥ÄÎ†®ÏÑ± Ï†êÏàò ÏàúÏúºÎ°ú Ï†ïÎ†¨
        sorted_files = sorted(found_files, key=lambda x: x["relevance_score"], reverse=True)
        total_found = len(sorted_files)
        
        # ÌéòÏù¥ÏßÄÎÑ§Ïù¥ÏÖò Ï†ÅÏö©
        paginated_files = sorted_files[request.skip:request.skip + request.limit]
        
        # 5. ÏùëÎãµ Î™®Îç∏Ïóê ÎßûÍ≤å Î≥ÄÌôò
        search_results = []
        for file_data in paginated_files:
            search_results.append(FileSearchResult(**file_data))
        
        execution_time = time.time() - start_time
        
        # ÎîîÎ≤ÑÍ∑∏ Ï†ïÎ≥¥ Î°úÍπÖ
        logger.info(f"Í≤ÄÏÉâ ÏôÑÎ£å - ÏøºÎ¶¨: '{request.query}', ÌÉÄÏûÖ: {request.search_type}, Ìè¥Îçî: {request.folder_id}, Í≤∞Í≥º: {total_found}Í∞ú")
        
        return FileSearchResponse(
            files=search_results,
            total_found=total_found,
            query=request.query,
            search_type=request.search_type,
            execution_time=round(execution_time, 3)
        )
        
    except Exception as e:
        logger.error(f"ÌååÏùº Í≤ÄÏÉâ Ïã§Ìå®: {e}")
        raise HTTPException(status_code=500, detail=f"ÌååÏùº Í≤ÄÏÉâ Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {str(e)}")

@router.delete("/{file_id}")
async def delete_file(file_id: str):
    """ÌååÏùº ÏôÑÏ†Ñ ÏÇ≠Ï†ú - Î™®Îì† Íµ¨Ï°∞Î•º ÏßÄÏõêÌïòÎäî ÌÜµÌï© ÏÇ≠Ï†ú"""
    try:
        db = await get_database()
        
        deleted_items = {
            "documents": 0,
            "chunks": 0,
            "file_info": 0,
            "labels": 0
        }
        
        logger.info(f"ÌååÏùº ÏÇ≠Ï†ú ÏãúÏûë: {file_id}")
        
        # 1. documents Ïª¨Î†âÏÖòÏóêÏÑú ÏÇ≠Ï†ú (ÏÉà Íµ¨Ï°∞ + Í∏∞Ï°¥ Íµ¨Ï°∞ Î™®Îëê ÏßÄÏõê)
        # ÏÉàÎ°úÏö¥ Íµ¨Ï°∞ (file_metadata.file_id)
        doc_result_new = await db.documents.delete_many({"file_metadata.file_id": file_id})
        deleted_items["documents"] += doc_result_new.deleted_count
        
        # Í∏∞Ï°¥ Íµ¨Ï°∞ (file_id ÏßÅÏ†ë)
        doc_result_old = await db.documents.delete_many({"file_id": file_id})
        deleted_items["documents"] += doc_result_old.deleted_count
        
        # 2. chunks Ïª¨Î†âÏÖòÏóêÏÑú ÏÇ≠Ï†ú
        chunks_result = await db.chunks.delete_many({"file_id": file_id})
        deleted_items["chunks"] = chunks_result.deleted_count
        
        # 3. file_info Ïª¨Î†âÏÖòÏóêÏÑú ÏÇ≠Ï†ú
        file_info_result = await db.file_info.delete_many({"file_id": file_id})
        deleted_items["file_info"] = file_info_result.deleted_count
        
        # 4. labels Ïª¨Î†âÏÖòÏóêÏÑú ÏÇ≠Ï†ú
        labels_result = await db.labels.delete_many({"document_id": file_id})
        deleted_items["labels"] = labels_result.deleted_count
        
        # 5. Í∏∞ÌÉÄ Ïª¨Î†âÏÖòÎì§ Ï†ïÎ¶¨ (ÏóêÎü¨Í∞Ä ÎÇòÎèÑ Í≥ÑÏÜç ÏßÑÌñâ)
        try:
            # summaries, qapairs, recommendationsÏóêÏÑú ÌòπÏãú file_id Ï∞∏Ï°∞ Ï†úÍ±∞
            await db.summaries.delete_many({"file_id": file_id})
            await db.qapairs.delete_many({
                "$or": [
                    {"file_id": file_id},
                    {"source": file_id}
                ]
            })
            await db.recommendations.delete_many({"file_id": file_id})
        except Exception as e:
            logger.warning(f"Í∏∞ÌÉÄ Ïª¨Î†âÏÖò Ï†ïÎ¶¨ Ï§ë Ïò§Î•ò (Î¨¥Ïãú): {e}")
        
        # 6. ÏûÑÏãú ÌååÏùº ÏÇ≠Ï†ú
        try:
            upload_dir = Path("uploads")
            deleted_files = []
            for temp_file in upload_dir.glob(f"{file_id}_*"):
                temp_file.unlink()
                deleted_files.append(str(temp_file))
            if deleted_files:
                logger.info(f"ÏûÑÏãú ÌååÏùº ÏÇ≠Ï†ú: {deleted_files}")
        except Exception as e:
            logger.warning(f"ÏûÑÏãú ÌååÏùº ÏÇ≠Ï†ú Ïã§Ìå® (Î¨¥Ïãú): {e}")
        
        # 7. Í≤∞Í≥º Í≤ÄÏ¶ù Î∞è ÏùëÎãµ
        total_deleted = sum(deleted_items.values())
        
        logger.info(f"ÌååÏùº ÏÇ≠Ï†ú ÏôÑÎ£å: {file_id}")
        logger.info(f"ÏÇ≠Ï†úÎêú Ìï≠Î™©Îì§: {deleted_items}")
        
        if total_deleted == 0:
            # ÏÇ≠Ï†úÌï† Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏúºÎ©¥ 404
            raise HTTPException(status_code=404, detail="ÏÇ≠Ï†úÌï† ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.")
        
        return {
            "success": True,
            "message": f"ÌååÏùºÏù¥ ÏôÑÏ†ÑÌûà ÏÇ≠Ï†úÎêòÏóàÏäµÎãàÎã§. (Ï¥ù {total_deleted}Í∞ú Ìï≠Î™©)",
            "file_id": file_id,
            "deleted_counts": deleted_items,
            "total_deleted": total_deleted
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ÌååÏùº ÏÇ≠Ï†ú Ïã§Ìå®: {e}")
        raise HTTPException(status_code=500, detail=f"ÌååÏùº ÏÇ≠Ï†ú Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {str(e)}")

@router.get("/list")
async def list_files(folder_id: Optional[str] = None, limit: int = 50, skip: int = 0):
    """ÏóÖÎ°úÎìúÎêú ÌååÏùº Î™©Î°ù Ï°∞Ìöå"""
    try:
        db = await get_database()
        
        # ÌïÑÌÑ∞ Ï°∞Í±¥
        filter_dict = {}
        if folder_id:
            filter_dict["folder_id"] = folder_id
        
        # Î¨∏ÏÑú Î™©Î°ù Ï°∞Ìöå (ÏÉàÎ°úÏö¥ Íµ¨Ï°∞Ïóê ÎßûÎäî Ï†ïÎ†¨)
        cursor = db.documents.find(filter_dict).sort("created_at", -1).skip(skip).limit(limit)
        documents = await cursor.to_list(None)
        
        # Í∞Å Î¨∏ÏÑúÏùò Ï≤≠ÌÅ¨ Í∞úÏàò Ï°∞Ìöå
        result = []
        for doc in documents:
            file_metadata = doc.get("file_metadata", {})
            file_id = file_metadata.get("file_id")
            
            if not file_id:
                continue
                
            chunks_count = await db.chunks.count_documents({"file_id": file_id})
            
            result.append({
                "file_id": file_id,
                "original_filename": file_metadata.get("original_filename", "Ïïå Ïàò ÏóÜÎäî ÌååÏùº"),
                "file_type": file_metadata.get("file_type", "unknown"),
                "file_size": file_metadata.get("file_size", 0),
                "processed_chunks": chunks_count,
                "upload_time": doc.get("created_at", datetime.utcnow()),
                "folder_id": doc.get("folder_id"),
                "description": file_metadata.get("description")
            })
        
        return {
            "files": result,
            "total": len(result),
            "skip": skip,
            "limit": limit
        }
        
    except Exception as e:
        logger.error(f"ÌååÏùº Î™©Î°ù Ï°∞Ìöå Ïã§Ìå®: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/semantic-search")
async def semantic_search_files(
    q: str,  # Í≤ÄÏÉâÏñ¥
    k: int = 5,  # Í≤∞Í≥º Í∞úÏàò
    folder_id: Optional[str] = None
):
    """AI Í∏∞Î∞ò ÏùòÎØ∏ Í≤ÄÏÉâ - Î≤°ÌÑ∞ Ïú†ÏÇ¨ÎèÑÎ°ú ÌååÏùº Ï∞æÍ∏∞"""
    try:
        # Í≤ÄÏÉâÏñ¥ Ïú†Ìö®ÏÑ± Í≤ÄÏÇ¨
        if not q or not q.strip():
            raise HTTPException(status_code=400, detail="Í≤ÄÏÉâÏñ¥Í∞Ä ÌïÑÏöîÌï©ÎãàÎã§.")
        
        # Í≤ÄÏÉâÏñ¥ Ï†ïÎ¶¨
        query = q.strip()
        logger.info(f"ÏãúÎß®Ìã± Í≤ÄÏÉâ ÏãúÏûë - ÏøºÎ¶¨: '{query}', k: {k}, folder_id: {folder_id}")
        
        db = await get_database()
        vector_search = VectorSearch(db)
        
        # ÌïÑÌÑ∞ Ï°∞Í±¥ ÏÑ§Ï†ï
        filter_dict = {}
        if folder_id and folder_id.strip():  # NoneÏù¥Í±∞ÎÇò Îπà Î¨∏ÏûêÏó¥Ïù¥ ÏïÑÎãê ÎïåÎßå
            filter_dict["folder_id"] = folder_id
        
        # Î≤°ÌÑ∞ Í≤ÄÏÉâ Ïã§Ìñâ
        search_results = await vector_search.search_similar(
            query=query,
            k=k * 3,  # Îçî ÎßéÏù¥ Ï∞æÏïÑÏÑú ÌååÏùºÎ≥ÑÎ°ú Í∑∏Î£πÌôî
            filter_dict=filter_dict
        )
        
        # ÌååÏùºÎ≥ÑÎ°ú Í∑∏Î£πÌôîÌïòÍ≥† ÏµúÍ≥† Ï†êÏàòÎßå ÎÇ®Í∏∞Í∏∞
        file_groups = {}
        for result in search_results:
            chunk = result.get("chunk", {})
            document = result.get("document", {})
            score = result.get("score", 0.0)
            file_id = chunk.get("file_id")
            
            if file_id and (file_id not in file_groups or score > file_groups[file_id]["relevance_score"]):
                # chunks Í∞úÏàò Ï°∞Ìöå
                chunks_count = await db.chunks.count_documents({"file_id": file_id})
                
                file_groups[file_id] = {
                    "file_id": file_id,
                    "original_filename": document.get("original_filename", "Ïïå Ïàò ÏóÜÎäî ÌååÏùº"),
                    "file_type": document.get("file_type", "unknown"),
                    "file_size": document.get("file_size", 0),
                    "processed_chunks": chunks_count,
                    "upload_time": document.get("upload_time"),
                    "folder_id": document.get("folder_id"),
                    "description": document.get("description"),
                    "match_type": "semantic",
                    "relevance_score": score,
                    "matched_content": chunk.get("text", "")[:200] + "..."
                }
        
        # Ï†êÏàò ÏàúÏúºÎ°ú Ï†ïÎ†¨ÌïòÍ≥† ÏÉÅÏúÑ kÍ∞úÎßå Î∞òÌôò
        sorted_files = sorted(file_groups.values(), key=lambda x: x["relevance_score"], reverse=True)
        top_files = sorted_files[:k]
        
        # ÏùëÎãµ ÏÉùÏÑ±
        search_results = [FileSearchResult(**file_data) for file_data in top_files]
        
        logger.info(f"ÏãúÎß®Ìã± Í≤ÄÏÉâ ÏôÑÎ£å - {len(search_results)}Í∞ú Í≤∞Í≥º")
        
        return FileSearchResponse(
            files=search_results,
            total_found=len(search_results),
            query=query,
            search_type="semantic",
            execution_time=0.0  # Ïã§Ï†ú ÏãúÍ∞Ñ Ï∏°Ï†ïÏùÄ ÏÉùÎûµ
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ÏùòÎØ∏ Í≤ÄÏÉâ Ïã§Ìå®: {e}")
        raise HTTPException(status_code=500, detail=f"ÏùòÎØ∏ Í≤ÄÏÉâ Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {str(e)}")

@router.get("/content/{file_id}")
async def get_file_content(file_id: str):
    """ÌååÏùºÏùò ÏõêÎ≥∏ ÌÖçÏä§Ìä∏ ÎÇ¥Ïö© Ï°∞Ìöå (ÌÜ†Í∏Ä ÌëúÏãúÏö©)"""
    try:
        db = await get_database()
        
        # documents Ïª¨Î†âÏÖòÏóêÏÑú ÌååÏùº Ï†ïÎ≥¥ Î∞è ÌÖçÏä§Ìä∏ Ï°∞Ìöå (ÏÉàÎ°úÏö¥ Íµ¨Ï°∞)
        document = await db.documents.find_one(
            {"file_metadata.file_id": file_id},
            {"file_metadata": 1, "raw_text": 1, "created_at": 1}
        )
        
        if not document:
            raise HTTPException(status_code=404, detail="ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.")
        
        file_metadata = document.get("file_metadata", {})
        
        return {
            "file_id": file_id,
            "original_filename": file_metadata.get("original_filename", "Ïïå Ïàò ÏóÜÎäî ÌååÏùº"),
            "file_type": file_metadata.get("file_type", "unknown"),
            "upload_time": document.get("created_at", datetime.utcnow()),
            "raw_text": document.get("raw_text", ""),
            "processed_text": document.get("raw_text", ""),  # ÏÉà Íµ¨Ï°∞ÏóêÏÑúÎäî ÎèôÏùº
            "text_length": len(document.get("raw_text", ""))
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ÌååÏùº ÎÇ¥Ïö© Ï°∞Ìöå Ïã§Ìå®: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.put("/{file_id}")
async def update_file_info(file_id: str, request: FileUpdateRequest):
    """ÌååÏùº Ï†ïÎ≥¥ ÏóÖÎç∞Ïù¥Ìä∏ (ÌååÏùºÎ™Ö, ÏÑ§Î™Ö, Ìè¥Îçî Îì±)"""
    try:
        db = await get_database()
        from bson import ObjectId
        
        # folder_idÏôÄ folder_title ÎèôÏãú ÏÇ¨Ïö© Î∞©ÏßÄ
        if request.folder_id and request.folder_title:
            raise HTTPException(
                status_code=400, 
                detail="folder_idÏôÄ folder_titleÏùÄ ÎèôÏãúÏóê ÏûÖÎ†•Ìï† Ïàò ÏóÜÏäµÎãàÎã§. Îëò Ï§ë ÌïòÎÇòÎßå ÏÑ†ÌÉùÌï¥Ï£ºÏÑ∏Ïöî."
            )
        
        # ÏóÖÎç∞Ïù¥Ìä∏Ìï† ÌïÑÎìú Ï§ÄÎπÑ
        update_fields = {}
        file_metadata_updates = {}
        
        if request.filename is not None and request.filename.strip():
            file_metadata_updates["file_metadata.original_filename"] = request.filename.strip()
        
        if request.description is not None:
            description_value = request.description.strip() if request.description.strip() else None
            file_metadata_updates["file_metadata.description"] = description_value
        
        # Ìè¥Îçî Ï≤òÎ¶¨
        final_folder_id = None
        
        # folder_id ÏßÅÏ†ë ÏûÖÎ†•
        if request.folder_id is not None:
            folder_id_input = request.folder_id.strip() if request.folder_id.strip() else None
            
            if folder_id_input and folder_id_input not in ["string", "null"]:
                # ObjectId Ïú†Ìö®ÏÑ± Í≤ÄÏ¶ù
                if not ObjectId.is_valid(folder_id_input):
                    raise HTTPException(status_code=400, detail="Ïú†Ìö®ÌïòÏßÄ ÏïäÏùÄ folder_id ÌòïÏãùÏûÖÎãàÎã§.")
                
                # Ìè¥Îçî Ï°¥Ïû¨ ÌôïÏù∏
                folder_exists = await db.folders.find_one({"_id": ObjectId(folder_id_input)})
                if not folder_exists:
                    raise HTTPException(status_code=404, detail=f"folder_id '{folder_id_input}'Ïóê Ìï¥ÎãπÌïòÎäî Ìè¥ÎçîÎ•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.")
                
                final_folder_id = folder_id_input
            else:
                final_folder_id = None  # Ìè¥Îçî Ìï¥Ï†ú
            
            update_fields["folder_id"] = final_folder_id
        
        # folder_titleÎ°ú Ìè¥Îçî Í≤ÄÏÉâ
        elif request.folder_title is not None:
            folder_title_input = request.folder_title.strip() if request.folder_title.strip() else None
            
            if folder_title_input and folder_title_input not in ["string", "null"]:
                # Ìè¥Îçî titleÎ°ú Í≤ÄÏÉâ
                folder_by_title = await db.folders.find_one({"title": folder_title_input})
                if not folder_by_title:
                    raise HTTPException(
                        status_code=404, 
                        detail=f"'{folder_title_input}' Ï†úÎ™©Ïùò Ìè¥ÎçîÎ•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§."
                    )
                
                final_folder_id = str(folder_by_title["_id"])
                update_fields["folder_id"] = final_folder_id
                logger.info(f"Ìè¥Îçî title '{folder_title_input}' -> folder_id: {final_folder_id}")
            else:
                final_folder_id = None  # Ìè¥Îçî Ìï¥Ï†ú
                update_fields["folder_id"] = final_folder_id
        
        # ÏóÖÎç∞Ïù¥Ìä∏Ìï† ÎÇ¥Ïö©Ïù¥ ÏûàÎäîÏßÄ ÌôïÏù∏
        all_updates = {**update_fields, **file_metadata_updates}
        if not all_updates:
            raise HTTPException(status_code=400, detail="ÏóÖÎç∞Ïù¥Ìä∏Ìï† ÎÇ¥Ïö©Ïù¥ ÏóÜÏäµÎãàÎã§.")
        
        # ÌååÏùº Ï°¥Ïû¨ ÌôïÏù∏ (ÏÉàÎ°úÏö¥ Íµ¨Ï°∞Ïóê ÎßûÍ≤å ÏàòÏ†ï)
        document = await db.documents.find_one({"file_metadata.file_id": file_id})
        if not document:
            raise HTTPException(status_code=404, detail="ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.")
        
        # Î¨∏ÏÑú ÏóÖÎç∞Ïù¥Ìä∏ (ÏÉàÎ°úÏö¥ Íµ¨Ï°∞Ïóê ÎßûÍ≤å ÏøºÎ¶¨ ÏàòÏ†ï)
        result = await db.documents.update_one(
            {"file_metadata.file_id": file_id},
            {"$set": all_updates}
        )
        
        if result.modified_count == 0:
            logger.warning(f"ÌååÏùº Ï†ïÎ≥¥ ÏóÖÎç∞Ïù¥Ìä∏ Í≤∞Í≥º ÏóÜÏùå: {file_id}")
        
        # Ìè¥Îçî Î≥ÄÍ≤ΩÏãú chunksÏùò metadataÎèÑ ÏóÖÎç∞Ïù¥Ìä∏
        if "folder_id" in update_fields:
            await db.chunks.update_many(
                {"file_id": file_id},
                {"$set": {"metadata.folder_id": update_fields["folder_id"]}}
            )
        
        # ÏÑ±Í≥µ Î©îÏãúÏßÄ ÏÉùÏÑ±
        updated_info = []
        if request.filename:
            updated_info.append(f"ÌååÏùºÎ™Ö: {request.filename}")
        if request.description is not None:
            updated_info.append(f"ÏÑ§Î™Ö: {request.description or '(Ï†úÍ±∞)'}")
        if final_folder_id:
            try:
                folder_info = await db.folders.find_one({"_id": ObjectId(final_folder_id)})
                folder_name = folder_info["title"] if folder_info else final_folder_id
                updated_info.append(f"Ìè¥Îçî: {folder_name}")
            except:
                updated_info.append(f"Ìè¥Îçî: {final_folder_id}")
        elif "folder_id" in update_fields and not final_folder_id:
            updated_info.append("Ìè¥Îçî: (Ìï¥Ï†ú)")
        
        success_message = "ÌååÏùº Ï†ïÎ≥¥Í∞Ä ÏóÖÎç∞Ïù¥Ìä∏ÎêòÏóàÏäµÎãàÎã§."
        if updated_info:
            success_message += f" ({', '.join(updated_info)})"
        
        logger.info(f"ÌååÏùº Ï†ïÎ≥¥ ÏóÖÎç∞Ïù¥Ìä∏ ÏôÑÎ£å: {file_id} - {all_updates}")
        
        return {
            "success": True,
            "message": success_message,
            "updated_fields": all_updates,
            "file_id": file_id
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ÌååÏùº Ï†ïÎ≥¥ ÏóÖÎç∞Ïù¥Ìä∏ Ïã§Ìå®: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/preview/{file_id}")
async def get_file_preview(file_id: str, max_length: int = 500):
    """ÌååÏùº ÎØ∏Î¶¨Î≥¥Í∏∞ - Ï≤òÏùå Î™á Ï§ÑÏùò ÌÖçÏä§Ìä∏Î•º Î∞òÌôò"""
    try:
        db = await get_database()
        
        # documents Ïª¨Î†âÏÖòÏóêÏÑú ÌååÏùº Ï†ïÎ≥¥ Ï°∞Ìöå (ÏÉàÎ°úÏö¥ Íµ¨Ï°∞Ïóê ÎßûÍ≤å ÏàòÏ†ï)
        document = await db.documents.find_one(
            {"file_metadata.file_id": file_id},
            {"file_metadata": 1, "raw_text": 1}
        )
        
        if not document:
            raise HTTPException(status_code=404, detail="ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.")
        
        file_metadata = document.get("file_metadata", {})
        raw_text = document.get("raw_text", "")
        file_type = file_metadata.get("file_type", "unknown")
        total_length = len(raw_text)
        
        # ÎØ∏Î¶¨Î≥¥Í∏∞ ÌÖçÏä§Ìä∏ ÏÉùÏÑ±
        preview_text = ""
        preview_type = "text"
        
        if raw_text:
            # ÌÖçÏä§Ìä∏Î•º Ï§Ñ Îã®ÏúÑÎ°ú Î∂ÑÌï†ÌïòÏó¨ ÏûêÏó∞Ïä§Îü¨Ïö¥ ÎØ∏Î¶¨Î≥¥Í∏∞ ÏÉùÏÑ±
            lines = raw_text.split('\n')
            current_length = 0
            preview_lines = []
            
            # Ï≤´ Î≤àÏß∏ Ï§ÑÏù¥ ÎÑàÎ¨¥ Í∏∏Î©¥ Îã®ÏàúÌûà ÏûòÎùºÏÑú ÏÇ¨Ïö©
            if lines and len(lines[0]) > max_length:
                preview_text = raw_text[:max_length] + "..."
            else:
                # Ï§Ñ Îã®ÏúÑÎ°ú Ï∂îÍ∞Ä
                for line in lines:
                    if current_length + len(line) + 1 > max_length:  # +1 for newline
                        break
                    preview_lines.append(line)
                    current_length += len(line) + 1
                
                preview_text = '\n'.join(preview_lines)
                
                # ÎßåÏïΩ preview_textÍ∞Ä Ïó¨Ï†ÑÌûà ÎπÑÏñ¥ÏûàÎã§Î©¥ Í∞ïÏ†úÎ°ú ÏùºÎ∂Ä ÌÖçÏä§Ìä∏ Ï∂îÍ∞Ä
                if not preview_text and raw_text:
                    preview_text = raw_text[:max_length] + ("..." if len(raw_text) > max_length else "")
            
            # ÌååÏùº ÌÉÄÏûÖÏóê Îî∞Î•∏ ÎØ∏Î¶¨Î≥¥Í∏∞ ÌÉÄÏûÖ Í≤∞Ï†ï
            if file_type == "pdf":
                preview_type = "pdf_extract"
            elif file_type in ["docx", "doc"]:
                preview_type = "document_extract"
            else:
                preview_type = "text"
        else:
            preview_text = "ÌÖçÏä§Ìä∏Î•º Ï∂îÏ∂úÌï† Ïàò ÏóÜÏäµÎãàÎã§."
        
        preview_length = len(preview_text)
        has_more = total_length > preview_length
        
        return FilePreviewResponse(
            file_id=file_id,
            original_filename=file_metadata.get("original_filename", "Ïïå Ïàò ÏóÜÎäî ÌååÏùº"),
            file_type=file_type,
            preview_text=preview_text,
            preview_length=preview_length,
            total_length=total_length,
            has_more=has_more,
            preview_type=preview_type
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ÌååÏùº ÎØ∏Î¶¨Î≥¥Í∏∞ Ïã§Ìå®: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/preview/chunks/{file_id}")
async def get_file_preview_with_chunks(file_id: str, chunk_count: int = 3):
    """Ï≤≠ÌÅ¨ Í∏∞Î∞ò ÌååÏùº ÎØ∏Î¶¨Î≥¥Í∏∞ - Ï≤òÏùå Î™á Í∞ú Ï≤≠ÌÅ¨Ïùò ÎÇ¥Ïö©"""
    try:
        db = await get_database()
        
        # ÌååÏùº Í∏∞Î≥∏ Ï†ïÎ≥¥ Ï°∞Ìöå (ÏÉàÎ°úÏö¥ Íµ¨Ï°∞Ïóê ÎßûÍ≤å ÏàòÏ†ï)
        document = await db.documents.find_one(
            {"file_metadata.file_id": file_id},
            {"file_metadata": 1}
        )
        
        if not document:
            raise HTTPException(status_code=404, detail="ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.")
        
        file_metadata = document.get("file_metadata", {})
        
        # Ï≤òÏùå Î™á Í∞ú Ï≤≠ÌÅ¨ Ï°∞Ìöå
        chunks_cursor = db.chunks.find(
            {"file_id": file_id}
        ).sort("sequence", 1).limit(chunk_count)
        
        chunks = await chunks_cursor.to_list(None)
        
        if not chunks:
            raise HTTPException(status_code=404, detail="ÌååÏùºÏùò Ï≤≠ÌÅ¨Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.")
        
        # Ï≤≠ÌÅ¨Îì§Ïùò ÌÖçÏä§Ìä∏Î•º Ìï©Ï≥êÏÑú ÎØ∏Î¶¨Î≥¥Í∏∞ ÏÉùÏÑ±
        preview_texts = [chunk["text"] for chunk in chunks]
        preview_text = "\n\n--- Îã§Ïùå ÏÑπÏÖò ---\n\n".join(preview_texts)
        
        # Ï†ÑÏ≤¥ Ï≤≠ÌÅ¨ Ïàò Ï°∞Ìöå
        total_chunks = await db.chunks.count_documents({"file_id": file_id})
        
        return {
            "file_id": file_id,
            "original_filename": file_metadata.get("original_filename", "Ïïå Ïàò ÏóÜÎäî ÌååÏùº"),
            "file_type": file_metadata.get("file_type", "unknown"),
            "preview_text": preview_text,
            "preview_chunks": len(chunks),
            "total_chunks": total_chunks,
            "has_more": total_chunks > len(chunks),
            "preview_type": "chunks"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Ï≤≠ÌÅ¨ Í∏∞Î∞ò ÎØ∏Î¶¨Î≥¥Í∏∞ Ïã§Ìå®: {e}")
        raise HTTPException(status_code=500, detail=str(e)) 